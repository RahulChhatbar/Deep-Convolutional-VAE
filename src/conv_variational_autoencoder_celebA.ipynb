{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4696841-996b-4c36-b508-81814bce242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1851460c-d5d8-4f8c-a2cf-cc7b9dc656fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e09130d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCVAE(nn.Module):\n",
    "    def __init__(self, input_dim=12288, hidden_dim=1000, latent_dim=128):\n",
    "        super(FCVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim * 4, latent_dim)\n",
    "\n",
    "        # Latent to hidden\n",
    "        self.z_fc = nn.Linear(latent_dim, hidden_dim * 4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc5 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc6 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        h2 = torch.relu(self.fc2(h1))\n",
    "        h3 = torch.relu(self.fc3(h2))\n",
    "        return self.fc_mu(h3), self.fc_logvar(h3)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.z_fc(z))\n",
    "        h = torch.relu(self.fc4(h))\n",
    "        h = torch.relu(self.fc5(h))\n",
    "        x_recon = torch.sigmoid(self.fc6(h))\n",
    "        return x_recon.view(-1, 3, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4272f59-12f5-4e44-a55b-75d4a25710d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv VAE Architecture\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.conv_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.z = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.debatch_norm1 = nn.BatchNorm2d(128)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        self.debatch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
    "        self.debatch_norm3 = nn.BatchNorm2d(32)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.leaky_relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.leaky_relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.leaky_relu(self.batch_norm4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.conv_mu(x)\n",
    "        logvar = self.conv_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.z(z)\n",
    "        x = x.view(-1, 256, 4, 4)\n",
    "        x = self.leaky_relu(self.debatch_norm1(self.deconv1(x)))\n",
    "        x = self.leaky_relu(self.debatch_norm2(self.deconv2(x)))\n",
    "        x = self.leaky_relu(self.debatch_norm3(self.deconv3(x)))\n",
    "        x = self.tanh(self.deconv4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e787790-a39d-4ada-bc09-e08879e61b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar, beta=0.001):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta * KLD, MSE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8566fc1e-df62-4a62-9ca4-df7aa7f83cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "def train(model, train_loader, optimizer, epoch, num_epochs, model_name, device):\n",
    "    model.train()\n",
    "    train_loss_total = 0\n",
    "    recon_loss_total = 0\n",
    "    kld_loss_total = 0\n",
    "    with tqdm(train_loader, unit='batch', desc=f'Epoch {epoch}', colour='green') as tepoch:\n",
    "        for batch_idx, (data, _) in enumerate(tepoch):\n",
    "            if model_name == 'cvae':\n",
    "                data = data.to(device)\n",
    "            elif model_name == 'fcvae':\n",
    "                data = data.view(data.size(0), -1).to(device)   \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            if model_name == 'fcvae':\n",
    "                recon_batch = recon_batch.view(data.size(0), -1)\n",
    "                data = data.view(data.size(0), -1)\n",
    "            total_loss, recon_loss, kld_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            total_loss.backward()\n",
    "            train_loss_total += total_loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kld_loss_total += kld_loss.item()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(\n",
    "                train_loss=train_loss_total / len(train_loader.dataset),\n",
    "                recon_loss=recon_loss_total / len(train_loader.dataset),\n",
    "                kld_loss=kld_loss_total / len(train_loader.dataset)\n",
    "            )\n",
    "    print(f'Epoch {epoch}/{num_epochs} train loss: {train_loss_total / len(train_loader.dataset)} mse loss: {recon_loss_total / len(train_loader.dataset)} kld loss: {kld_loss_total / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a7986f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d55a1-0fdd-4943-ba97-39c83e85719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Train the CVAE\n",
    "transform_cvae = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_dataset_cvae = datasets.CelebA('./data2/cvae', split='train', download=True, transform=transform_cvae)\n",
    "train_loader_cvae = torch.utils.data.DataLoader(train_dataset_cvae, batch_size=128, shuffle=True)\n",
    "\n",
    "cvae = ConvVAE(latent_dim=128).to(device)\n",
    "optimizer_cvae = optim.Adam(cvae.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(cvae, train_loader_cvae, optimizer_cvae, epoch, num_epochs, 'cvae', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the FCVAE\n",
    "transform_fcvae = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_dataset_fcvae = datasets.CelebA('./data2/fcvae', split='train', download=True, transform=transform_fcvae)\n",
    "train_loader_fcvae = torch.utils.data.DataLoader(train_dataset_fcvae, batch_size=128, shuffle=True)\n",
    "\n",
    "fcvae = FCVAE(latent_dim=128).to(device)\n",
    "optimizer_fcvae = optim.Adam(fcvae.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(fcvae, train_loader_fcvae, optimizer_fcvae, epoch, num_epochs, 'fcvae', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7560b0ff-44c3-40ac-9434-8b32b489c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and display new images\n",
    "def generate_images(model, model_name, num_images, latent_dim):\n",
    "    if np.sqrt(num_images) % 1 != 0:\n",
    "        raise ValueError('num_images must be a perfect square')\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Turn off gradients for generation\n",
    "        # Sample random points from the latent space (standard normal distribution)\n",
    "        z = torch.randn(num_images, latent_dim).to(next(model.parameters()).device)\n",
    "        # Decode these points to generate images\n",
    "        generated_images = []\n",
    "        for i in tqdm(range(num_images), desc='Generating images', unit='image', colour='blue'):\n",
    "            generated_image = model.decode(z[i:i+1]).cpu()\n",
    "            generated_images.append(generated_image)\n",
    "    # Convert the generated images to a single tensor\n",
    "    generated_images = torch.cat(generated_images, dim=0)\n",
    "\n",
    "    if model_name == 'cvae':\n",
    "        # Rescale the images to the range [-1, 1]\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "\n",
    "    # Rescale the images to the range [0, 1]\n",
    "    generated_images = torch.clamp(generated_images, 0, 1)\n",
    "\n",
    "    grid_size = int(np.sqrt(num_images))\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(grid_size + 1, grid_size + 1))\n",
    "    axs = axs.flatten() if len(axs.shape) == 2 else axs\n",
    "    for i in range(num_images):\n",
    "        axs[i].imshow(generated_images[i].numpy().transpose(1, 2, 0))\n",
    "        axs[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f8932-f0c9-4a47-ac17-4bc93e8ae945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "print('Images from FC VAE')\n",
    "generate_images(fcvae, 'fcvae', num_images=25, latent_dim=128)\n",
    "\n",
    "print('Images from Conv VAE')\n",
    "generate_images(cvae, 'cvae', num_images=25, latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "450b05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate between two images\n",
    "def interpolate_images(model, model_name, image1, image2, device, num_steps=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "        if model_name == 'fcvae':\n",
    "            image1 = image1.view(-1, 3*32*32)\n",
    "            image2 = image2.view(-1, 3*32*32)\n",
    "        mu1, logvar1 = model.encode(image1.unsqueeze(0))\n",
    "        mu2, logvar2 = model.encode(image2.unsqueeze(0))\n",
    "        z1 = model.reparameterize(mu1, logvar1)\n",
    "        z2 = model.reparameterize(mu2, logvar2)\n",
    "        interpolated_images = []\n",
    "        for alpha in np.linspace(0, 1, num_steps):\n",
    "            interpolated_z = alpha * z1 + (1 - alpha) * z2\n",
    "            reconstructed_image = model.decode(interpolated_z).cpu()\n",
    "            interpolated_images.append(reconstructed_image)\n",
    "            if model_name == 'cvae':\n",
    "                reconstructed_image = (reconstructed_image + 1) / 2\n",
    "        interpolated_images = torch.cat(interpolated_images, dim=0)\n",
    "        return torch.clamp(interpolated_images, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad194c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_interpolation(model, model_name, dataset, num_steps=10):\n",
    "    image1, _ = dataset[0]\n",
    "    image2, _ = dataset[1]\n",
    "    interpolated_images = interpolate_images(model, model_name, image1, image2, device, num_steps=num_steps)\n",
    "    fig, axs = plt.subplots(1, num_steps, figsize=(num_steps, 1))\n",
    "    for i in range(num_steps):\n",
    "        axs[i].imshow(interpolated_images[i].numpy().transpose(1, 2, 0))\n",
    "        axs[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67770a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between two images from the dataset for Conv VAE\n",
    "visualize_interpolation(cvae, 'cvae', train_dataset_cvae, num_steps=10)\n",
    "\n",
    "# Interpolate between two images from the dataset for FC VAE\n",
    "visualize_interpolation(fcvae, 'fcvae', train_dataset_fcvae, num_steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
